![image-20250611091053075](C:\Users\FanYang\AppData\Roaming\Typora\typora-user-images\image-20250611091053075.png)

### 一、基础交互

- `Press p to pause, r to resume, q to quit.`
  工具交互指令，用于控制监控流程：按 `p` 暂停、`r` 恢复、`q` 退出，方便动态观测程序状态。
- `Status: Running`
  程序当前状态为 “运行中”，指标实时更新。

### 二、指标分类解析（按功能分组）

#### 1. **垃圾回收（GC）相关**

| 指标名称                         | 当前值       | 含义 & 分析建议                                              |
| -------------------------------- | ------------ | ------------------------------------------------------------ |
| `% Time in GC since last GC (%)` | 0%           | 上一次 GC 周期内，GC 耗时占总时间的比例。0% 说明近期 GC 几乎无耗时，内存压力小。 |
| `Allocation Rate (B / 2 sec)`    | 98,016 B     | 每 2 秒的内存分配总量（约 95.7KB/2 秒 ）。低分配率说明对象创建少，GC 触发频率低。 |
| `GC Committed Bytes (MB)`        | 0 MB         | GC 堆已向操作系统 “提交” 的内存（实际占用的物理 / 虚拟内存）。0 可能是工具采样误差或程序刚启动，需结合场景看。 |
| `GC Fragmentation (%)`           | 1.985%       | 堆碎片率（碎片 = 不可用的小内存块总和 / 堆总大小）。1.985% 较低，堆空间利用较高效。 |
| `GC Heap Size (MB)`              | 203 MB       | 当前 GC 堆中**已分配对象**的总内存（不包含空闲块）。反映程序实际内存占用规模。 |
| `Gen 0 GC Count (Count / 2 sec)` | 0            | 每 2 秒内第 0 代（年轻对象）GC 次数。0 代回收最频繁，0 次说明近期无年轻对象需回收。 |
| `Gen 0 Size (B)`                 | 768 B        | 第 0 代堆的当前大小（年轻对象总内存）。768B 很小，说明无大量短期对象。 |
| `Gen 1 GC Count (Count / 2 sec)` | 0            | 每 2 秒内第 1 代 GC 次数。1 代是 0 代回收后存活的对象，0 次说明无 1 代对象需回收。 |
| `Gen 1 Size (B)`                 | 1,570,504 B  | 第 1 代堆大小（约 1.5MB ）。若持续增长，需关注对象是否异常存活（本该释放却进入 1 代）。 |
| `Gen 2 GC Count (Count / 2 sec)` | 0            | 每 2 秒内第 2 代（老年代）GC 次数。2 代回收频率最低，0 次说明老年代内存稳定。 |
| `Gen 2 Size (B)`                 | 1.8188e+08 B | 第 2 代堆大小（约 173.5MB ）。老年代存长期存活对象，若持续增长需排查内存泄漏（如静态集合未释放）。 |

#### 2. **CPU 与代码执行**

| 指标名称                         | 当前值       | 含义 & 分析建议                                              |
| -------------------------------- | ------------ | ------------------------------------------------------------ |
| `CPU Usage (%)`                  | 11%          | 程序占用 CPU 的比例。11% 说明有一定计算负载，若业务逻辑需要则正常，异常时需查死循环、高频计算。 |
| `IL Bytes Jitted (B)`            | 11,329,055 B | JIT（实时编译）的 IL 代码总字节数（约 10.8MB ）。反映 JIT 编译的代码规模，越大说明动态编译越多。 |
| `Time spent in JIT (ms / 2 sec)` | 0 ms         | 每 2 秒内 JIT 编译耗时。0 说明近期无新代码编译，程序处于 “稳态” 执行已编译逻辑。 |

#### 3. **内存细节（大对象、固定对象）**

| 指标名称                            | 当前值       | 含义 & 分析建议                                              |
| ----------------------------------- | ------------ | ------------------------------------------------------------ |
| `LOH Size (B)`                      | 16,418,520 B | 大对象堆（LOH，默认 >85KB 的对象）大小（约 15.7MB ）。LOH 不参与 0/1 代回收，需关注大对象创建逻辑。 |
| `POH (Pinned Object Heap) Size (B)` | 91,913 B     | 固定对象堆（POH）大小（约 89.8KB ）。被 “固定” 的对象无法被 GC 移动，可能导致碎片，需查 `fixed` 语句、P/Invoke 场景。 |

#### 4. **线程与并发**

| 指标名称                                               | 当前值 | 含义 & 分析建议                                              |
| ------------------------------------------------------ | ------ | ------------------------------------------------------------ |
| `Monitor Lock Contention Count (Count / 2 sec)`        | 47     | 每 2 秒内锁竞争次数。47 次说明有一定锁争抢，需查高并发场景下的锁粒度（如用 `Concurrent` 集合替代锁）。 |
| `Number of Active Timers`                              | 179    | 活跃定时器数量。179 个可能较多，需排查是否有重复创建、未释放的定时器，避免定时任务堆积。 |
| `ThreadPool Completed Work Item Count (Count / 2 sec)` | 226    | 每 2 秒内线程池完成的任务数。反映线程池吞吐量，226 次说明任务处理较频繁。 |
| `ThreadPool Queue Length`                              | 0      | 线程池等待队列长度。0 说明无任务排队，线程池线程充足，任务处理及时。 |
| `ThreadPool Thread Count`                              | 77     | 线程池当前活跃线程数。77 个说明程序用多线程并行处理任务，需结合 CPU 核心数判断是否合理（避免过度线程）。 |

#### 5. **其他辅助指标**

| 指标名称                          | 当前值    | 含义 & 分析建议                                              |
| --------------------------------- | --------- | ------------------------------------------------------------ |
| `Exception Count (Count / 2 sec)` | 0         | 每 2 秒抛出的异常数量。0 说明无异常抛出，程序执行稳定；若高频非 0，需查异常处理逻辑（别用异常控制流程）。 |
| `Number of Assemblies Loaded`     | 252       | 已加载的程序集数量。252 个说明依赖较多，需关注程序集加载耗时、是否有冗余依赖。 |
| `Number of Methods Jitted`        | 1,704,064 | 已 JIT 编译的方法数量。方法越多，JIT 耗时可能越高，可考虑 AOT 编译优化启动性能。 |
| `Working Set (MB)`                | 1,446 MB  | 程序当前占用的物理内存（含堆、栈、程序集等）。反映进程整体内存 footprint，需结合机器内存判断是否合理。 |

### 三、整体总结

这组数据反映程序处于 **“低 GC 压力、稳定执行”** 状态：



- 内存分配率低（95.7KB/2 秒 ）、GC 耗时占比 0%，说明对象创建少，内存管理高效；
- CPU 占用 11% 适中，线程池无任务排队，并发模型较健康；
- 若需进一步优化，可关注：
  - 老年代（Gen 2）内存规模（173.5MB ），长期运行需确认是否有内存泄漏；
  - 活跃定时器数量（179 个），排查是否有不必要的定时任务；
  - 大对象堆（LOH，15.7MB ），确认大对象创建是否合理（如拆分大数组）。



简言之，当前程序运行稳定，若需极致优化，可针对老年代内存、定时器、大对象等细节深入排查～

# `Net看板分析`

### 1、请求平均持续时间

![image-20250702093132748](C:\Users\FanYang\AppData\Roaming\Typora\typora-user-images\image-20250702093132748.png)

# 下面是分析内容：

这是一张 **请求平均持续时间（按操作类型）的时序图** ，用于监控系统中不同操作的响应耗时情况，可从以下维度分析：

### 1. 核心指标与场景

- **指标**：`Average request duration`（请求平均持续时间），反映各操作的响应耗时，直接关联系统性能与用户体验。
- **场景**：属于系统性能监控范畴，用于定位慢操作、分析系统负载高峰的性能表现，是排查接口超时、优化用户体验的关键依据。

### 2. 数据趋势与异常点

- **整体趋势**：图中存在明显的 “阶梯式” 耗时增长，推测对应系统负载上升（如流量高峰、任务调度触发），多数操作耗时随负载增加而上升，体现系统性能受压力影响的关联关系。
- **异常点**：初始阶段（左侧）耗时接近 `0`，应为系统空闲或无真实请求；某一时刻（竖线处）耗时突然飙升，是流量 / 任务开始触发的关键节点，需结合业务日志确认对应事件（如定时任务启动、用户访问高峰）。

### 3. 操作类型对比

- **耗时分层**：不同 `action`（操作）的耗时曲线分层明显，部分操作（如 `CheckBlackList` 等）长期处于高耗时区间（20 - 40 秒），属于 **“慢操作”** ，需重点优化；部分操作（如 `GetBasFunctionByDefine` 等）耗时相对较低（10 秒内），性能表现更优。
- **关键操作**：高耗时操作（如 `CheckBlackList`、`GetBasFunction` 等）是性能优化的核心对象，需排查其业务逻辑复杂度、依赖资源（数据库、第三方服务）的响应效率；低耗时操作可作为基准，对比分析高耗时操作的差异点（如是否涉及复杂计算、多表关联查询）。

### 4. 优化与排查方向

- 慢操作优化

  ：针对高耗时操作，建议：

  - 检查 **业务逻辑**：是否存在不必要的循环、冗余计算；
  - 分析 **依赖资源**：关联的数据库查询是否未加索引、第三方接口是否响应慢；
  - 压测验证：模拟高负载场景，确认耗时是否随并发上升而线性增长，判断是否存在资源瓶颈（如线程池满、数据库连接池不足）。

- **监控延伸**：结合 `请求成功率`、`并发量` 等指标，分析耗时与流量的关联关系（如高耗时是否由并发过高导致）；对高耗时操作增加 **详细链路追踪**（如分布式 tracing），定位具体耗时阶段（如接口调用、数据库查询、缓存读写）。

### 5. 总结与价值

这张图清晰呈现了系统操作的性能基线与负载影响，通过分析可快速锁定慢操作、关联业务场景，为性能优化提供明确方向。核心价值在于：



- **问题定位**：快速识别影响用户体验的慢操作；
- **根因分析**：关联业务场景（如流量高峰）与系统性能的关系；
- **优化验证**：后续优化（如接口重构、数据库调优）后，可通过同图表对比，验证耗时是否下降，量化优化效果。



若需进一步深入，建议补充 **操作的业务逻辑描述**、**系统架构图（如依赖的服务、数据库类型）** ，可更精准分析耗时根源～

# **系统请求相关的监控数据**

![image-20250702093437335](C:\Users\FanYang\AppData\Roaming\Typora\typora-user-images\image-20250702093437335.png)

### 下面是分析内容：

### 1. “Total req/s”（每秒总请求数 ）

- **含义**：反映系统每秒接收的请求总量，体现流量压力。
- **分析**：曲线波动说明流量有变化，峰值约 110 req/s ，代表系统在对应时刻承受的请求负载，可结合业务（如促销、定时任务）看流量来源，也用于判断系统是否因流量过高引发性能问题（如与前图高耗时是否同时间段）。

### 2. “In progress (glob.)”（进行中请求数 ）

- **含义**：实时显示当前未完成、正在处理的请求数量，体现系统当前 “在途” 负载。
- **分析**：数值 17 及曲线波动，说明系统持续有请求在处理。若该数值持续高位且伴随前图高耗时，可能是系统处理能力不足（如线程池满、资源瓶颈），导致请求堆积；若数值低但耗时高，更倾向于单个请求本身处理慢（如复杂逻辑、依赖外部服务超时 ）。



整体看，这两组数据需结合前图 “请求耗时” 关联分析：若 “Total req/s” 高峰时，“In progress” 持续上升且 “请求耗时” 变长，基本可判断系统因流量超过处理能力，引发请求堆积和性能下降，是排查系统吞吐量、资源瓶颈的关键依据 。



#  **按控制器（Controller）分类的请求接收量（Requests received by controller）时序堆积图**

![image-20250702093749298](C:\Users\FanYang\AppData\Roaming\Typora\typora-user-images\image-20250702093749298.png)

### 下面是分析内容：

这是一张 **按控制器（Controller）分类的请求接收量（Requests received by controller）时序堆积图** ，用于展示不同业务控制器的请求流量分布与趋势，以下逐维度拆解：

### 1. 核心指标与场景

- **指标**：`Requests received by controller`（按控制器接收的请求量），反映各业务模块的流量负载，是分析系统流量分布、定位高负载模块的关键依据。
- **场景**：属于系统流量监控范畴，用于识别业务高峰的流量热点、验证功能模块的使用热度（如某新功能是否被高频访问），也可结合 `响应耗时` 分析高流量模块的性能压力。

### 2. 维度拆解：控制器（Controller）

图中不同颜色对应 **业务控制器**（可理解为不同功能模块的入口），每个 `Controller` 的含义与典型场景：



| 控制器名称               | 可能的业务含义（推测，需结合实际代码）                   | 典型场景示例                     |
| ------------------------ | -------------------------------------------------------- | -------------------------------- |
| `BarcodeScan`            | 条码扫描相关功能（如扫码录入、条码校验）                 | 生产车间扫码工序、仓库出入库扫码 |
| `InterfaceprdModelRoute` | 产品模型路由（可能关联产品配置、流程跳转）               | 产品配置变更、流程节点切换       |
| `PassStation`            | 工作站 / 关卡校验（如生产工位校验、流程节点准入）        | 生产流程中工位权限校验           |
| `PrdSN`                  | 产品序列号（SN）管理（查询、录入、校验）                 | 产品追溯、SN 绑定流程            |
| `ProcessPack`            | 流程打包 / 工序打包（如生产流程打包、数据批量处理）      | 生产批次数据汇总、工序批量提交   |
| `RtPrdSntrace`           | 产品序列号追溯（简化版，可能侧重基础轨迹查询）           | 快速查询产品关键节点轨迹         |
| `RtPrdSntraceDetail`     | 产品序列号详细追溯（含全链路数据，如操作人、时间、参数） | 售后故障排查、审计级追溯需求     |

### 3. 流量趋势与分布

- **整体趋势**：流量随时间呈现 “波动上升 - 高峰持续 - 逐步回落” 的规律，推测对应业务流程的高峰时段（如生产班次的集中操作、定时任务触发前的准备）。不同控制器的流量曲线形态相似，说明业务流程存在 **关联性**（如 `PrdSN` 操作可能触发 `ProcessPack` 流程）。

- 流量分层

  ：

  - **高流量层**：`PassStation`（浅蓝）、`PrdSN`（橙色）等控制器长期处于流量顶部，是系统的 **核心业务入口** ，需重点关注其性能（如是否因高流量导致响应慢）；
  - **低流量层**：`RtPrdSntrace`（蓝）、`RtPrdSntraceDetail`（紫）等流量较低，可能对应 “追溯查询” 这类低频但关键的操作（如仅售后或审计使用）。

### 4. 关键分析方向

- **流量热点验证**：若某新功能对应的 `Controller`（如 `InterfaceprdModelRoute`）流量远低于预期，需排查功能是否未正确推广、入口是否隐蔽；若某老功能（如 `PrdSN`）持续高流量，可结合业务数据验证是否为 “核心流程”。
- **性能关联分析**：结合前文的 `请求耗时图` ，若某高流量控制器（如 `PassStation`）同时伴随 **高响应耗时** ，则需优先优化（高流量 + 慢响应会直接影响用户体验与业务效率）。
- **异常检测**：若某控制器流量 **突然偏离基线**（如 `ProcessPack` 平时流量稳定，某时段骤增），需结合业务日志排查是否触发异常流程（如批量任务误执行、用户操作失误）。

### 5. 总结与价值

这张图的核心价值在于 **“流量 - 业务模块” 的映射** ：



- 帮助团队快速识别 **业务核心模块**（高流量控制器），集中资源优化其性能与稳定性；
- 验证 **业务流程设计**（如各控制器的流量占比是否符合预期，判断功能使用是否与设计目标一致）；
- 结合其他指标（如 `响应耗时` `错误率`），构建完整的 **“流量 - 性能 - 业务” 关联分析体系** ，定位问题更精准（如 “高流量模块是否同时有高报错”）。



若需进一步精准分析，建议补充：



- 实际代码中 `Controller` 对应的 **业务逻辑文档**（确认功能含义）；
- 同期的 **业务操作日志**（如生产班次、用户操作记录），验证流量高峰与业务行为的关联。



# `total_connections`（Kestrel 服务器总连接数）监控图的参数及含义做详细拆解

![image-20250702094142902](C:\Users\FanYang\AppData\Roaming\Typora\typora-user-images\image-20250702094142902.png)

### 下面是分析内容：

以下对这张 `total_connections`（Kestrel 服务器总连接数）监控图的参数及含义做详细拆解，帮你理解系统连接数的分布与趋势：

### 一、核心指标：`total_connections`

`total_connections` 代表 **Kestrel 服务器（.NET 生态中常用的 Web 服务器）的总连接数** ，反映系统实时的网络连接负载，是诊断服务端网络性能、排查连接泄漏 / 瓶颈的关键指标，数值越高通常意味着并发访问或长连接场景越密集。

### 二、维度拆解：每条曲线的参数含义

图中不同颜色曲线对应 **带标签（label）的监控指标** ，每个标签参数的意义：



| 参数（标签）                                                 | 含义说明                                                     | 作用与价值                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `__name__="microsoft_aspnetcore_server_kestrel_total_connections"` | 指标名称，固定标识这是 Kestrel 服务器的 “总连接数” 指标      | 明确监控对象，区分其他类型指标（如请求耗时、错误数等）       |
| `instance="10.2.255.15:30038"` `instance="10.2.255.15:30032"` 等 | 实例标识，`10.2.255.15` 是服务器 IP ，`30038`/`30032` 是 Kestrel 服务监听端口 | 定位具体的服务实例，排查问题时可精准对应到某台服务器、某个端口的服务 |
| `job="centerservice"`                                        | 任务 / 作业标识，说明这些实例都属于 “centerservice” 这个业务服务（需结合实际部署定义，通常关联服务名） | 按业务归类，方便区分不同服务的连接数（如同时监控 “用户服务”“订单服务”，可通过 `job` 筛选） |
| 颜色（可视化层）                                             | 无实际参数意义，仅用于图表中区分不同实例的曲线               | 直观对比多实例的连接数趋势，快速发现某实例连接数异常（如某条曲线突然飙升，对应实例可能有流量突增） |

### 三、趋势与分析价值

1. **整体趋势**：
   所有实例的连接数随时间 **稳步上升** ，说明系统整体并发连接在增长，可能是业务流量上升（如用户访问量增加、定时任务触发长连接），或存在 **连接未及时释放** 的隐患（需结合业务逻辑判断是否合理）。
   若连接数持续高位且不回落，需警惕 **连接池耗尽**（影响新请求接入）、**资源泄漏**（如代码未正确关闭连接）。
2. **实例对比**：
   不同 `instance`（实例）的连接数曲线有差异，说明各实例承担的流量 / 连接负载不同。若某实例连接数 **远高于其他实例** ，需排查：
   - 该实例是否被负载均衡策略优先转发（如权重配置错误）；
   - 该实例是否关联特殊业务（如专属接口、高频访问功能）。
3. **结合业务场景**：
   - 若业务是 “生产管理系统”，连接数高峰可能对应 **生产班次操作**（如工人集中扫码、提交数据）；
   - 若连接数持续上升但业务流量无明显变化，需检查代码中 `HttpClient` 、长连接配置（如 SignalR 、WebSocket ）是否存在 **连接未释放** 问题。

### 四、典型排查方向

1. **连接泄漏**：
   若连接数长期上升不回落，结合代码检查：
   - 是否有 `IDisposable` 资源（如 `HttpClient` 、数据库连接）未正确 `Dispose`；
   - 长连接场景（如 WebSocket ）是否未处理 `Close` 事件，导致连接堆积。
2. **负载均衡验证**：
   若实例间连接数差异过大，检查负载均衡策略（如 Nginx 、K8s Service 配置），确认是否按预期均匀分配流量。
3. **业务关联分析**：
   提取连接数高峰时段，对比业务日志（如用户操作记录、任务调度日志），验证是否为正常业务流量（如批量数据上传、定时同步任务），排除异常流量（如爬虫、攻击）。



简单说，这张图通过 `Kestrel 总连接数` 反映系统网络连接负载，参数帮你定位 **“哪个服务实例、属于哪个业务、在什么网络位置”** ，趋势则帮你发现连接数异常（泄漏、不均），是诊断服务端网络性能、排查连接相关问题的基础依据 。



# **系统接口（action）的平均请求耗时**

![image-20250702160056001](C:\Users\FanYang\AppData\Roaming\Typora\typora-user-images\image-20250702160056001.png)

#### 这是一张**Prometheus + Grafana 监控图表**，用于分析**系统接口（action）的平均请求耗时**，核心信息拆解如下

### 1. 整体维度

- **指标含义**：`Average request duration / per action`（每个操作的平均请求耗时），反映系统不同功能的响应性能。
- **时间范围**：图表横轴是时间（15:35–15:50 左右），纵轴是耗时（从 0 到 1 分钟），监控粒度为分钟级。

### 2. 关键动作（action）分析

图表通过不同颜色线，区分系统的核心功能（action），重点看耗时突出的行为：



- **`GetDataSetEncrypted`（粉红色线）**：
  - 是耗时**最高的功能**，大部分时间接近 1 分钟（60 秒），偶有波动。
  - 推测：可能是涉及**加密数据集查询**的操作，因加密、大数据量或复杂计算，导致响应慢，需重点优化（如异步处理、缓存、算法优化）。
- **其他动作**：
  - 如 `Add`（黄色）、`CheckRole`（紫色）等，耗时普遍低于 40 秒，属于相对 “轻量” 或优化较好的功能。
  - 部分动作（如 `AddTableUpdateLog` 蓝色、`BasFunctionExist` 橙色）耗时更低（接近 20 秒内），属于高频、低耗时的基础功能。

### 3. 系统性能结论

- **核心问题点**：`GetDataSetEncrypted` 是性能瓶颈，长期高耗时可能引发用户体验差、接口超时（若系统设置超时阈值）。

- 优化建议

  ：

  - 优先排查 `GetDataSetEncrypted`：检查是否有不必要的加密逻辑、数据查询是否可分页 / 缓存，或是否存在资源竞争（如数据库锁、线程阻塞）。
  - 监控趋势：观察高耗时动作是否持续稳定（如 `GetDataSetEncrypted` 始终高耗时，需重构；若偶发，排查特定时间点的系统负载）。

### 4. 技术实现补充

- **指标来源**：基于 Prometheus 的 `http_request_duration_seconds_bucket` 指标（通过 `increase` 函数计算区间增量），属于**直方图（Histogram）类型**监控，可分析请求耗时的分布。
- **可视化工具**：Grafana 图表（典型 PromQL 语法 + 多指标聚合），适合团队长期监控系统性能、定位瓶颈。



总结：这张图清晰暴露了系统的**加密数据查询功能性能问题**，需聚焦 `GetDataSetEncrypted` 做深度优化；其他功能耗时相对可控，可作为常态化性能监控持续观察。